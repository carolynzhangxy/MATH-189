{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200e3bab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Week 9\n",
    "## MATH 189 • Data Analysis & Inference • Wi 2024\n",
    "### Siddharth Vishwanath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700143e-29e1-4307-9662-02a03e22ef20",
   "metadata": {},
   "source": [
    "% # %load tex-macros\n",
    "<div hidden>\n",
    "\\newcommand{\\require}[1]{}\n",
    "\n",
    "$\\require{begingroup}\\require{newcommand}$\n",
    "$\\long\\def \\forcecommand #1{\\providecommand{#1}{}\\renewcommand{#1}}$\n",
    "$\\forcecommand{\\defeq}{\\stackrel{\\small\\bullet}{=}}$\n",
    "$\\forcecommand{\\ra}{\\rangle}$\n",
    "$\\forcecommand{\\la}{\\langle}$\n",
    "$\\forcecommand{\\pr}{{\\mathbb P}}$\n",
    "$\\forcecommand{\\qr}{{\\mathbb Q}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\av}{{\\boldsymbol{a}}}$\n",
    "$\\forcecommand{\\bv}{{\\boldsymbol{b}}}$\n",
    "$\\forcecommand{\\cv}{{\\boldsymbol{c}}}$\n",
    "$\\forcecommand{\\dv}{{\\boldsymbol{d}}}$\n",
    "$\\forcecommand{\\ev}{{\\boldsymbol{e}}}$\n",
    "$\\forcecommand{\\fv}{{\\boldsymbol{f}}}$\n",
    "$\\forcecommand{\\gv}{{\\boldsymbol{g}}}$\n",
    "$\\forcecommand{\\hv}{{\\boldsymbol{h}}}$\n",
    "$\\forcecommand{\\nv}{{\\boldsymbol{n}}}$\n",
    "$\\forcecommand{\\sv}{{\\boldsymbol{s}}}$\n",
    "$\\forcecommand{\\tv}{{\\boldsymbol{t}}}$\n",
    "$\\forcecommand{\\uv}{{\\boldsymbol{u}}}$\n",
    "$\\forcecommand{\\vv}{{\\boldsymbol{v}}}$\n",
    "$\\forcecommand{\\wv}{{\\boldsymbol{w}}}$\n",
    "$\\forcecommand{\\zerov}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\onev}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\phiv}{{\\boldsymbol{\\phi}}}$\n",
    "$\\forcecommand{\\cc}{{\\check{C}}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\Xv}{{\\boldsymbol{X}\\!}}$\n",
    "$\\forcecommand{\\yv}{{\\boldsymbol{y}}}$\n",
    "$\\forcecommand{\\Yv}{{\\boldsymbol{Y}}}$\n",
    "$\\forcecommand{\\zv}{{\\boldsymbol{z}}}$\n",
    "$\\forcecommand{\\Zv}{{\\boldsymbol{Z}}}$\n",
    "$\\forcecommand{\\Iv}{{\\boldsymbol{I}}}$\n",
    "$\\forcecommand{\\Jv}{{\\boldsymbol{J}}}$\n",
    "$\\forcecommand{\\Cv}{{\\boldsymbol{C}}}$\n",
    "$\\forcecommand{\\Ev}{{\\boldsymbol{E}}}$\n",
    "$\\forcecommand{\\Fv}{{\\boldsymbol{F}}}$\n",
    "$\\forcecommand{\\Gv}{{\\boldsymbol{G}}}$\n",
    "$\\forcecommand{\\Hv}{{\\boldsymbol{H}}}$\n",
    "$\\forcecommand{\\alphav}{{\\boldsymbol{\\alpha}}}$\n",
    "$\\forcecommand{\\epsilonv}{{\\boldsymbol{\\epsilon}}}$\n",
    "$\\forcecommand{\\betav}{{\\boldsymbol{\\beta}}}$\n",
    "$\\forcecommand{\\deltav}{{\\boldsymbol{\\delta}}}$\n",
    "$\\forcecommand{\\gammav}{{\\boldsymbol{\\gamma}}}$\n",
    "$\\forcecommand{\\etav}{{\\boldsymbol{\\eta}}}$\n",
    "$\\forcecommand{\\piv}{{\\boldsymbol{\\pi}}}$\n",
    "$\\forcecommand{\\thetav}{{\\boldsymbol{\\theta}}}$\n",
    "$\\forcecommand{\\tauv}{{\\boldsymbol{\\tau}}}$\n",
    "$\\forcecommand{\\muv}{{\\boldsymbol{\\mu}}}$\n",
    "$\\forcecommand{\\sd}{\\text{SD}}$\n",
    "$\\forcecommand{\\se}{\\text{SE}}$\n",
    "$\\forcecommand{\\med}{\\text{median}}$\n",
    "$\\forcecommand{\\median}{\\text{median}}$\n",
    "$\\forcecommand{\\Ber}{{\\text{Ber}}}$\n",
    "$\\forcecommand{\\Bin}{{\\text{Bin}}}$\n",
    "$\\forcecommand{\\Geo}{{\\text{Geo}}}$\n",
    "$\\forcecommand{\\Unif}{{\\text{Unif}}}$\n",
    "$\\forcecommand{\\Poi}{{\\text{Poi}}}$\n",
    "$\\forcecommand{\\Exp}{{\\text{Exp}}}$\n",
    "$\\forcecommand{\\Chisq}{{\\chi^2}}$\n",
    "$\\forcecommand{\\N}{\\mathbb{N}}$\n",
    "$\\forcecommand{\\iid}{{\\stackrel{iid}{\\sim}}}$\n",
    "$\\forcecommand{\\px}{p_{X}}$\n",
    "$\\forcecommand{\\fx}{f_{X}}$\n",
    "$\\forcecommand{\\Fx}{F_{X}}$\n",
    "$\\forcecommand{\\py}{p_{Y}}$\n",
    "$\\forcecommand{\\pxy}{p_{X,Y}}$\n",
    "$\\forcecommand{\\po}{{p_0}}$\n",
    "$\\forcecommand{\\pa}{{p_a}}$\n",
    "$\\forcecommand{\\Xbar}{\\overline{X}}$\n",
    "$\\forcecommand{\\Ybar}{\\overline{Y}}$\n",
    "$\\forcecommand{\\Zbar}{\\overline{Z}}$\n",
    "$\\forcecommand{\\nXbar}{n \\cdot \\overline{X}}$\n",
    "$\\forcecommand{\\nYbar}{n \\cdot \\overline{Y}}$\n",
    "$\\forcecommand{\\nZbar}{n \\cdot \\overline{Z}}$\n",
    "$\\forcecommand{\\Xn}{X_1, X_2, \\dots, X_n}$\n",
    "$\\forcecommand{\\Xm}{{X_1, X_2, \\dots, X_m}}$\n",
    "$\\forcecommand{\\Yn}{Y_1, Y_2, \\dots, Y_n}$\n",
    "$\\forcecommand{\\Ym}{{Y_1, Y_2, \\dots, Y_m}}$\n",
    "$\\forcecommand{\\sumXn}{X_1 + X_2 + \\dots + X_n}$\n",
    "$\\forcecommand{\\sumym}{Y_1 + Y_2 + \\dots + Y_m}$\n",
    "$\\forcecommand{\\la}{\\ell_\\alpha}$\n",
    "$\\forcecommand{\\ua}{u_\\alpha}$\n",
    "$\\forcecommand{\\at}{{\\alpha/2}}$\n",
    "$\\forcecommand{\\mux}{\\mu_{X}}$\n",
    "$\\forcecommand{\\muy}{\\mu_{Y}}$\n",
    "$\\forcecommand{\\sx}{\\sigma_{X}}$\n",
    "$\\forcecommand{\\sy}{\\sigma_{Y}}$\n",
    "$\\forcecommand{\\pvalue}{$p$-value}$\n",
    "$\\forcecommand{\\Ho}{H_{0}}$\n",
    "$\\forcecommand{\\Ha}{H_{a}}$\n",
    "$\\forcecommand{\\pvalue}{p\\text{-value}}$\n",
    "$\\forcecommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "\\vskip-\\parskip\n",
    "\\vskip-\\baselineskip\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96abc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Agenda\n",
    "---\n",
    "\n",
    "1. Ridge, Lasso, and Elastic Net\n",
    "\n",
    "2. Principal Component Analysis and Principal Component Regression\n",
    "\n",
    "3. Nonlinear and Nonparameteric Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ea4ab",
   "metadata": {},
   "source": [
    "Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f314af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Optional \n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62d02c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### The regression model\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_i \\quad \\text{where} \\quad \\epsilon_i \\iid (0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Equivalently, in matrix notation:\n",
    "\n",
    "$$\n",
    "\\yv = \\Xv \\betav + \\epsilonv\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae2ce6",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0daa895",
   "metadata": {},
   "source": [
    "## §1. Regularization / Shrinkage Methods\n",
    "\n",
    "* Regularization / Shrinkage methods are a class of methods that use a **penalty** to shrink the coefficients of the less important predictors to zero. \n",
    "\n",
    "* It achieves a similar objective to stepwise selection, but uses a slightly different strategy. \n",
    "\n",
    "\n",
    "To see this, first let's look at the objective function of the **standard regression** task:\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> #### Standard Regression\n",
    "\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\epsilon\n",
    "$$\n",
    "\n",
    "Recall that the **least-squares loss** selects the model with the smallest residual standard error, i.e., \n",
    "\n",
    "$$\n",
    "L(\\beta_0, \\beta_2, \\dots, \\beta_p) = SS_{Res} = \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_{1, i} - \\dots \\beta_p x_{p, i})^2\n",
    "$$\n",
    "\n",
    "\n",
    "The solution to this problem is denoted as follows:\n",
    "\n",
    "$$\n",
    "(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2, \\dots, \\hat\\beta_p) = \\mathop{\\arg\\min}\\limits_{\\beta_1 \\dots \\beta_p} L(\\beta_0, \\beta_2, \\dots, \\beta_p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c43134",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c8b6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> #### Regularized Regression\n",
    "\n",
    "If we want to select only a subset of these variables in our final model, we can include a penalty term \n",
    "$$p_\\lambda(\\beta_1, \\dots, \\beta_p)$$ \n",
    "\n",
    "which **favours solutions which select smaller subsets of the variables.** In this setting, the objective function becomes\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "L_\\lambda(\\beta_0, \\beta_1, \\dots, \\beta_p) = L(\\beta_0, \\beta_2, \\dots, \\beta_p) + p_\\lambda(\\beta_1, \\dots, \\beta_p)\n",
    "}\n",
    "$$\n",
    "\n",
    "* Here $\\lambda$ is a user-defined parameter that controls the strength of the penalty.\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0274c64",
   "metadata": {},
   "source": [
    "> #### What do we want an ideal penalty function to do?\n",
    "\n",
    "\n",
    "1. We want $p_\\lambda(\\beta_1, \\beta_2, \\dots, \\beta_p)$ to be be **large** when too many predictors are included in the model. \n",
    "\n",
    "2. We want $p_\\lambda(\\beta_1, \\beta_2, \\dots, \\beta_p)$ to be **small** when only a few predictors are included in the model.\n",
    "\n",
    "3. Ideally, we want $p_\\lambda(\\beta_1, \\beta_2, \\dots, \\beta_p)$ to be **zero** when only the true predictors are included in the model.\n",
    "    * Unfortunately, this ideal is never achieved in practice! \n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c3016",
   "metadata": {},
   "source": [
    "> #### The regularization trade-off\n",
    "\n",
    "The final solution to the regularized regression problem is denoted as follows:\n",
    "\n",
    "$$\n",
    "(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2, \\dots, \\hat\\beta_p) = \\mathop{\\arg\\min}\\limits_{\\beta_1 \\dots \\beta_p} L(\\beta_0, \\beta_2, \\dots, \\beta_p) + p_\\lambda(\\beta_1, \\dots, \\beta_p)\n",
    "$$\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "The solution to this problem is a trade-off between:\n",
    "\n",
    "* the **model fit** since we are minimizing $L(\\beta_0, \\beta_2, \\dots, \\beta_p)$\n",
    "* the **complexity of the model** since we are also minimizing $p_\\lambda(\\beta_1, \\dots, \\beta_p)$\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "Observations:\n",
    "* If we only focus on $L(\\beta_0, \\beta_2, \\dots, \\beta_p)$, we will select too many variables.\n",
    "* If we only focus on $p_\\lambda(\\beta_1, \\dots, \\beta_p)$, we will end up selecting no predictors at all!\n",
    "* So the parameter $\\lambda$ controls the trade-off between the two objectives.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83017056",
   "metadata": {},
   "source": [
    "> #### 1. The Ridge Penalty\n",
    "\n",
    "The ridge penalty is defined as\n",
    "\n",
    "$$\n",
    "p_\\lambda(\\beta_1, \\dots, \\beta_p) = \\lambda ||{\\betav}||^2 =  \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "The ridge penalty is also known as the $L_2$ penalty. It favours solutions where all the coefficients are small.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "> #### 2. The Lasso Penalty\n",
    "\n",
    "The LASSO (Least Angle Shrinkage and Selection Operator) penalty is defined as\n",
    "\n",
    "$$\n",
    "p_\\lambda(\\beta_1, \\dots, \\beta_p) = \\lambda ||{\\betav}||_1 =  \\lambda \\sum_{j=1}^p |\\beta_j|\n",
    "$$\n",
    "\n",
    "The LASSO penalty is also known as the $L_1$ penalty. It favours solutions where many of the coefficients are zero.\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "> #### 3. The Elastic Net Penalty\n",
    "\n",
    "The elastic net penalty is a combination of the ridge and lasso penalties. It is defined as\n",
    "\n",
    "$$\n",
    "p_\\lambda(\\beta_1, \\dots, \\beta_p) = \\lambda \\Big( \\gamma ||{\\betav}||_1 + (1-\\gamma) ||{\\betav}||^2 \\Big)\n",
    "$$\n",
    "\n",
    "where $\\gamma$ controls how much of the penalty is due to the lasso penalty and $(1-\\gamma)$ controls how much of the penalty is due to the ridge penalty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0a6a9",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c55e2",
   "metadata": {},
   "source": [
    "In `statsmodels` you can use the `fit_regularized` method to fit a regularized regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cda86f",
   "metadata": {},
   "source": [
    "Let's look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92bcb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 100, 50\n",
    "true_p = np.random.choice(range(50), replace=False, size=5)\n",
    "true_b = np.array([2,3,4,5,6])\n",
    "print(f'true_p: {np.sort(true_p)}')\n",
    "\n",
    "x = np.random.randn(n, p)\n",
    "toy_df = pd.DataFrame(x, columns=[f'x{i}' for i in range(p)])\n",
    "toy_df[\"y\"] = x[:, true_p] @ true_b + np.random.randn(n)\n",
    "\n",
    "df = toy_df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c898c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = toy_df\n",
    "\n",
    "full_model_formula = f' y ~ {'+'.join(df.columns.drop('y'))}'\n",
    "\n",
    "elastic_fit = smf.ols(full_model_formula, data=df).\\\n",
    "                fit_regularized(method='elastic_net', alpha=1.0, L1_wt=1.0)\n",
    "\n",
    "print(elastic_fit.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baae846",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1wt = 1.0\n",
    "alphas = np.linspace(1e-3, 0.5, 200)\n",
    "elastic_coefs = []\n",
    "for alpha in alphas:\n",
    "    elastic_fit = smf.ols(full_model_formula, data=df).\\\n",
    "                fit_regularized(method='elastic_net', alpha=alpha, L1_wt=l1wt)\n",
    "    elastic_coefs.append(elastic_fit.params[1:])\n",
    "\n",
    "\n",
    "# Dataframe of coefficients\n",
    "elastic_coefs = pd.DataFrame(elastic_coefs, index=alphas)\n",
    "elastic_coefs\n",
    "\n",
    "\n",
    "# plot each coefficient\n",
    "# fig, ax = plt.subplots(figsize=(7, 7))\n",
    "# for column in elastic_coefs.columns:\n",
    "#     ax.plot(alphas, elastic_coefs[column], label=column)\n",
    "# ax.set_xlabel('alpha')\n",
    "# ax.set_ylabel('coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elastic_fit.params[elastic_fit.params != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05449dfd",
   "metadata": {},
   "source": [
    "**Consider the Boston housing data:**\n",
    "\n",
    "\n",
    "The original data are 506 observations on 14 variables, medv being the target variable:\n",
    "\n",
    "* `crim` per capita crime rate by town\n",
    "* `zn` proportion of residential land zoned for lots over 25,000 sq.ft\n",
    "* `indus` proportion of non-retail business acres per town\n",
    "* `chas` Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* `nox` nitric oxides concentration (parts per 10 million)\n",
    "* `rm` average number of rooms per dwelling\n",
    "* `age` proportion of owner-occupied units built prior to 1940\n",
    "* `dis` weighted distances to five Boston employment centres\n",
    "* `rad` index of accessibility to radial highways\n",
    "* `tax` full-value property-tax rate per USD 10,000\n",
    "* `ptratio` pupil-teacher ratio by town\n",
    "* ~`b` 1000(B - 0.63)^2 where B is the proportion of black residents by town~\n",
    "* `lstat` percentage of lower status of the population\n",
    "* `medv` median value of owner-occupied homes in USD 1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://vincentarelbundock.github.io/Rdatasets/csv/MASS/Boston.csv\"\n",
    "boston = pd.read_csv(url)\n",
    "df = boston\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e338ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = boston\n",
    "\n",
    "response = 'medv'\n",
    "full_model_formula = f' {response} ~ {'+'.join(df.columns.drop(response))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ab4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1wt = 1.0\n",
    "alphas = np.linspace(1e-3, 0.5, 200)\n",
    "elastic_coefs = []\n",
    "for alpha in alphas:\n",
    "    elastic_fit = smf.ols(full_model_formula, data=df).\\\n",
    "                fit_regularized(method='elastic_net', alpha=alpha, L1_wt=l1wt)\n",
    "    elastic_coefs.append(elastic_fit.params[1:])\n",
    "\n",
    "\n",
    "# Dataframe of coefficients\n",
    "elastic_coefs = pd.DataFrame(elastic_coefs, index=alphas)\n",
    "elastic_coefs\n",
    "\n",
    "# plot each coefficient\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "for column in elastic_coefs.columns:\n",
    "    ax.plot(alphas, elastic_coefs[column], label=column)\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fit = elastic_fit = smf.ols(full_model_formula, data=df).\\\n",
    "                fit_regularized(method='elastic_net', alpha=0.1, L1_wt=1.0)\n",
    "\n",
    "print(final_fit.params[final_fit.params != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce35a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = final_fit.params[final_fit.params != 0].index[2:]\n",
    "\n",
    "print(smf.ols( f'{response} ~ {\"+\".join(final_params)}', df).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f17aa",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6246137",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027484b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Intuition for Ridge and LASSO\n",
    "\n",
    "* In the absence of $p_\\lambda(\\beta_1, \\beta_2, \\dots, \\beta_p)$ the objective is to minimize the residual sum of squares loss function, i.e., \n",
    "\n",
    "$$\n",
    "\\mathop{\\min} L(\\beta_0, \\beta_2, \\dots, \\beta_p)\n",
    "$$\n",
    "\n",
    "\n",
    "* After adding the penalty term, the objective can equivalently be written as\n",
    "\n",
    "$$\n",
    "\\mathop{\\min} L(\\beta_0, \\beta_2, \\dots, \\beta_p)\\\\ \\\\\n",
    "s.t. \\quad p(\\beta_1, \\dots, \\beta_p) \\leq t\n",
    "$$\n",
    "\n",
    "where $t$ is inversely related to $\\lambda$, i.e. if $t$ is small then $\\lambda$ in $p_\\lambda(\\cdot)$ is large and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d448d",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38713d",
   "metadata": {},
   "source": [
    "Consider the following example of \n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760f68b",
   "metadata": {},
   "source": [
    "Suppose the values $(\\hat\\beta_1, \\hat\\beta_2)$ which minimize the loss function are:\n",
    "\n",
    "$$\n",
    "(\\hat\\beta_1, \\hat\\beta_2) = (1, 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01477b63",
   "metadata": {},
   "source": [
    "The loss function \n",
    "\n",
    "$$\n",
    "L(\\beta_1, \\beta_2) = \\sum_{i=1}^n (y_i - \\beta_1x_{1i} - \\beta_2 x_{2i})^2\n",
    "$$\n",
    "\n",
    "define ellipsoidal regions in the $\\beta_1, \\beta_2$ plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (1, 1)\n",
    "\n",
    "@interact(t=(0.01, 1.5, 0.01), l1wt=(0, 1, 0.1))\n",
    "def plot(t, l1wt):\n",
    "    x1seq = np.linspace(-3, 3, 500)\n",
    "    x2seq = np.linspace(-3, 3, 500)\n",
    "    X1, X2 = np.meshgrid(x1seq, x2seq)\n",
    "    d = np.array([X1.flatten(), X2.flatten()]).T\n",
    "\n",
    "    z = [ l1wt * (np.abs(x[0]) + np.abs(x[1])) + (1-l1wt) * (x[0]**2 + x[1]**2) < t for x in d]\n",
    "    Z = np.array(z).reshape(X1.shape)\n",
    "\n",
    "    y = [(x[0]-b[0])**2 / 3 + (x[1] - b[1])**2 /0.5 for x in d]\n",
    "    Y = np.array(y).reshape(X1.shape)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.axhline(0, color='black', lw=2)\n",
    "    ax.axvline(0, color='black', lw=2)\n",
    "    ax.contourf(X1, X2, Z, alpha=0.5)\n",
    "    ax.contour(X1, X2, Y, levels=np.linspace(0, 3, 10), colors='black', linestyles='dashed')\n",
    "    ax.set_xlabel('b1')\n",
    "    ax.set_ylabel('b2')\n",
    "    ax.set_title(f'Contour plot of $\\\\lambda$ = {l1wt: .3f} and $t$ = {t: .3f}')\n",
    "    ax.plot(b[0], b[1], 'ro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0f1c5",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25248541",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f494933",
   "metadata": {},
   "source": [
    "For a majority of this course we have focused on **supervised learning** where we have access to **labelled data** i.e., we are given access to the _covariates and the responses_\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{observation}\\ 1: &\\quad (X_{1, 1}, X_{2, 1}, \\dots X_{p, 1}, y_1)\\\\\n",
    "\\text{observation}\\ 2: &\\quad (X_{1, 2}, X_{2, 2}, \\dots X_{p, 2}, y_2)\\\\\n",
    "\\vdots\\quad & \\quad\\quad\\quad\\vdots\\\\ \n",
    "\\text{observation}\\ n: &\\quad (X_{1, n}, X_{2, n}, \\dots X_{p, n}, y_n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "Our **goal** has been to:\n",
    "\n",
    "* Predict $y$ using $X_1, X_2, \\dots X_p$\n",
    "* Understand how each $X_i$ influences the response $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf691b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In unsupervised learning we **DON'T** have access to the labelled data, i.e., we are only given:\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{observation}\\ 1: &\\quad (X_{1, 1}, X_{2, 1}, \\dots X_{p, 1})\\\\\n",
    "\\text{observation}\\ 2: &\\quad (X_{1, 2}, X_{2, 2}, \\dots X_{p, 2})\\\\\n",
    "\\vdots\\quad & \\quad\\quad\\quad\\vdots\\\\ \n",
    "\\text{observation}\\ n: &\\quad (X_{1, n}, X_{2, n}, \\dots X_{p, n})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "Our **goal** here is:\n",
    "* To understand the relationship between $X_1, X_2, \\dots X_p$\n",
    "    > * **dimension reduction**: \n",
    "    > \n",
    "    > Can we discover subgroups of variables $X_1, X_2, \\dots X_p$ which behave similarly?\n",
    "    \n",
    "    > * **clustering**:\n",
    "    >\n",
    "    > Can we discover subgroups of observations $1, 2, \\dots n$ which are similar?\n",
    "    \n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a3724",
   "metadata": {},
   "source": [
    "#### Why unsupervised learning?\n",
    "\n",
    "It is always easier to obtain unlabeled data as oppposed to labeled data (which require someone or something to actually assign the proper responses $y_1, y_2, \\dots y_n$)\n",
    "\n",
    "In statistics and data science, there are a multitude of different methods which have been proposed to tackle each of the two problems. They include:\n",
    "\n",
    "* **Dimension reduction**:\n",
    "    * Principal component analysis\n",
    "    * Uniform Manifold Approximation (UMAP)\n",
    "    * t-Stochastic Neighbor embedding (t-SNE)\n",
    "    * ...\n",
    "    \n",
    "* **Clustering**:\n",
    "    * k-means clustering\n",
    "    * Hierarchical clustering\n",
    "    * Topological clustering\n",
    "    * Laplacian eigenmaps\n",
    "    * ...\n",
    "    \n",
    "This is one of the most exciting parts of data-science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a2435",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2340cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## §2. Principal Component Analysis\n",
    "\n",
    "\n",
    "Given variables $(X_1, X_2, \\dots X_p)$, PCA produces a low-dimensional representation of the dataset, i.e., \n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{observation}\\ 1: &\\quad (X_{1, 1}, X_{2, 1}, \\dots X_{p, 1}) \\longrightarrow (Z_{1, 1}, Z_{2, 1})\\\\\n",
    "\\text{observation}\\ 2: &\\quad (X_{1, 2}, X_{2, 2}, \\dots X_{p, 2}) \\longrightarrow (Z_{1, 2}, Z_{2, 2})\\\\\n",
    "\\vdots\\quad & \\quad\\quad\\quad\\vdots\\\\ \n",
    "\\text{observation}\\ n: &\\quad (X_{1, n}, X_{2, n}, \\dots X_{p, n}) \\longrightarrow (Z_{1, n}, Z_{2, n})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "It tries to create variables $(Z_1, Z_2, \\dots Z_q)$ for $q < p$ such that:\n",
    "\n",
    "1. $q \\ll p$\n",
    "1. $(Z_1, Z_2, \\dots Z_q)$ contains _roughly_ the same information as $(X_1, X_2, \\dots X_p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1edfd",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138943d",
   "metadata": {},
   "source": [
    "#### How does PCA achieve this?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "##### Step 1:\n",
    "\n",
    "The **first principal component** $Z_1$ is the _normalized_ linear combination of the features:\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$\n",
    "Z_1 = v_{11} X_1 + v_{21} X_2 + \\dots v_{p1} X_p\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "such that: \n",
    "* $Z_1$ has the largest possible variance\n",
    "* $\\sum_{i=1}^p v^2_{p, i} = 1$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "> #### Note:\n",
    "> $V_1 = (v_{11}, v_{21}, \\dots v_{p1})$ are called the **factor loadings**\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "##### Step 2:\n",
    "\n",
    "The **second principal component** $Z_2$ is the _normalized_ linear combination of the features:\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$\n",
    "Z_2 = v_{12} X_1 + v_{22} X_2 + \\dots v_{p2} X_p\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "such that: \n",
    "* $V_2 \\perp V_1$\n",
    "* $Z_2$ has the largest possible variance\n",
    "* $\\sum_{i=1}^p v^2_{p, 2} = 1$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "$\\vdots\\\\\\vdots$\n",
    "\n",
    "---\n",
    "\n",
    "##### Step q:\n",
    "\n",
    "The **$q$th principal component** $Z_q$ is the _normalized_ linear combination of the features:\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$\n",
    "Z_2 = v_{12} X_1 + v_{22} X_2 + \\dots v_{p2} X_p\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "such that: \n",
    "* $Z_q$ has the largest possible variance\n",
    "* $V_q \\perp \\text{span}(V_1, V_2, \\dots, V_{q-1})$\n",
    "* $\\sum_{i=1}^p v^2_{p, 2} = 1$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f540ff",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172a5a2",
   "metadata": {},
   "source": [
    "> #### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aeaefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# muiltivariate normal data in R2 with 0.75 correlation\n",
    "np.random.seed(0)\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0.75], [0.75, 1]]\n",
    "data = np.random.multivariate_normal(mean, cov, 1000)\n",
    "df = pd.DataFrame(data, columns=['x1', 'x2'])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x1'], df['x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance matrix for x1 and x2\n",
    "C = np.cov(df['x1'], df['x2'])\n",
    "print(f'Covariance matrix:\\n {C}\\n')\n",
    "print(f'Total variance: {np.trace(C)}\\n\\n')\n",
    "\n",
    "r = np.corrcoef(df['x1'], df['x2'])\n",
    "print(f'Correlation: {r[0, 1]}\\n\\n')\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "\n",
    "print(f' Eigenvalues:\\n {eigenvalues}')\n",
    "print(f' Eigenvectors:\\n {eigenvectors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddfa93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x1'], df['x2'])\n",
    "ax.plot([0, eigenvectors[0, 0]], [0, eigenvectors[1, 0]], color='red', lw=2)\n",
    "ax.plot([0, eigenvectors[0, 1]], [0, eigenvectors[1, 1]], color='red', lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9057a",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2c8d4",
   "metadata": {},
   "source": [
    "Transformed coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6bfc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated = df @ eigenvectors.T\n",
    "rotated.columns = ['x1_rotated', 'x2_rotated']\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(rotated['x1_rotated'], rotated['x2_rotated'])\n",
    "ax.axhline(0, color='black', lw=2)\n",
    "ax.axvline(0, color='black', lw=2)\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_C = np.cov(rotated['x1_rotated'], rotated['x2_rotated'])\n",
    "print(f' Rotated covariance matrix:\\n {rotated_C}\\n\\n')\n",
    "print(f' Rotated total variance: \\n{np.trace(rotated_C)}\\n\\n')\n",
    "\n",
    "\n",
    "new_correlation = np.corrcoef(rotated['x1_rotated'], rotated['x2_rotated'])[0, 1]\n",
    "print(f' New correlation: {new_correlation}')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "math189",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc": {
   "base_numbering": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
